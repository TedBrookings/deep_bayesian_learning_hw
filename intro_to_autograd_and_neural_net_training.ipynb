{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to neural net training with autograd\n",
    "\n",
    "In this notebook, we'll practice\n",
    "\n",
    "* using the **autograd** Python package to compute gradients\n",
    "* using gradient descent to train a basic linear regression (a NN with 0 hidden layers)\n",
    "* using gradient descent to train a basic neural network for regression (NN with 1+ hidden layers)\n",
    "\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "Follow the Python environment setup instructions here:\n",
    "<https://www.cs.tufts.edu/comp/150BDL/2019f/setup_python_env.html>\n",
    "\n",
    "All the specific Python packages you'll need are in the 'bdl_2019_env' conda environment:\n",
    "<https://www.cs.tufts.edu/comp/150BDL/2019f/conda_envs/bdl_2019_env.yml>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import plotting tools\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import numpy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import autograd\n",
    "import autograd.numpy as ag_np\n",
    "import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.cos\n",
    "g = autograd.grad(f)\n",
    "print(g(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Using autograd's 'grad' function on univariate functions\n",
    "\n",
    "Suppose we have a mathematical function of interest $f(x)$. For now, we'll work with functions that have a scalar input and scalar output. \n",
    "\n",
    "Then we can of course ask: what is the derivative (aka *gradient*) of this function:\n",
    "\n",
    "$$\n",
    "g(x) \\triangleq \\frac{\\partial}{\\partial x} f(x)\n",
    "$$\n",
    "\n",
    "Instead of computing this gradient by hand via calculus/algebra, we can use autograd to do it for us.\n",
    "\n",
    "First, we need to implement the math function $f(x)$ as a **Python function** `f`.\n",
    "\n",
    "\n",
    "The Python function `f` needs to satisfy the following requirements:\n",
    "* INPUT 'x': scalar float\n",
    "* OUTPUT 'f(x)': scalar float\n",
    "* All internal operations are composed of calls to functions from `ag_np`, the `autograd` version of numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return ag_np.square(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a gradient function via `autograd.grad`\n",
    "\n",
    "Now, if `f` meeds the above requirements, we can create a Python function `g` to compute derivatives of $f(x)$ by calling `autograd.grad`:\n",
    "\n",
    "```\n",
    "g = autograd.grad(f)\n",
    "```\n",
    "\n",
    "The symbol `g` is now a **Python function** that takes the same input as `f`, but produces the derivative at a given input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the gradient function\n",
    "\n",
    "'g' is just a function. You can call it as usual, by providing a possible scalar float input\n",
    "\n",
    "### Example: What is the gradient of x^2 at x=0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: What is the gradient of x^2 at x=16?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(16.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: what if we provide an int type as input, not a float?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: we can use the results of gradient function calls inside Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[g(-1.0), g(1.0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: we can use the results of gradient calls inside Python dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(gradient_at_4=g(4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: A Note on Importing autograd \n",
    "\n",
    "* You might be used to importing numpy as `import numpy as np`, and then using this shorthand for `np.cos(0.0)` or `np.square(5.0)` etc.\n",
    "* For autograd to work, you need to instead use **autograd's** provided numpy wrapper interface: `import autograd.numpy as ag_np`\n",
    "* The `ag_np` module has the same API as `numpy`, so you can call `ag_np.cos(0.0)`, `ag_np.square(5.0)`, etc.\n",
    "* For this course, we like to be careful and specify when you need the `ag_np` module and when you can get away with `np`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot to demonstrate the gradient function  side-by-side with original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid_G = np.linspace(-10, 10, 100)\n",
    "\n",
    "fig_h, subplot_grid = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, squeeze=False)\n",
    "subplot_grid[0,0].plot(x_grid_G, [f(x_g) for x_g in x_grid_G], 'k.-')\n",
    "subplot_grid[0,0].set_title('f(x) = x^2')\n",
    "\n",
    "subplot_grid[0,1].plot(x_grid_G, [g(x_g) for x_g in x_grid_G], 'b.-')\n",
    "subplot_grid[0,1].set_title('gradient of f(x)'); # trailing semi-colon eats any stdout from this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1a:\n",
    "\n",
    "Consider the decaying periodic function below. Can you compute its derivative using autograd and plot the result?\n",
    "\n",
    "$$\n",
    "f(x) = e^{-x/10} * cos(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.0 # TODO compute the function above! Should you use \"np\" or \"ag_np\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda x: 0.0 # TODO define g as gradient of f, using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO plot the result\n",
    "x_grid_G = np.linspace(-10, 10, 500)\n",
    "fig_h, subplot_grid = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, squeeze=False)\n",
    "\n",
    "## Left panel\n",
    "subplot_grid[0,0].plot(x_grid_G, [f(x_g) for x_g in x_grid_G], 'k.-');\n",
    "subplot_grid[0,0].set_title('f(x) = e^{-x/10} * cos(x)');\n",
    "\n",
    "## Right panel\n",
    "subplot_grid[0,1].plot(x_grid_G, [g(x_g) for x_g in x_grid_G], 'b.-');\n",
    "subplot_grid[0,1].set_title('gradient of f(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Using autograd's 'grad' function on functions with multivariate input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine the input $x$ could be a vector of size D. \n",
    "\n",
    "Our mathematical function $f(x)$ will map each input vector to a scalar.\n",
    "\n",
    "We want the gradient function\n",
    "\n",
    "\\begin{align}\n",
    "g(x) &\\triangleq \\nabla_x f(x)\n",
    "\\\\\n",
    "&= [\n",
    "    \\frac{\\partial}{\\partial x_1} f(x)\n",
    "    \\quad \\frac{\\partial}{\\partial x_2} f(x)\n",
    "    \\quad \\ldots \\quad \\frac{\\partial}{\\partial x_D} f(x)  ]\n",
    "\\end{align}\n",
    "\n",
    "Instead of computing this gradient by hand via calculus/algebra, we can use autograd to do it for us.\n",
    "\n",
    "First, we implement math function $f(x)$ as a **Python function** `f`.\n",
    "\n",
    "The Python function `f` needs to satisfy the following requirements:\n",
    "* INPUT 'x': numpy array of float\n",
    "* OUTPUT 'f(x)': scalar float\n",
    "* All internal operations are composed of calls to functions from `ag_np`, the `autograd` version of numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x_D):\n",
    "    return ag_np.sum(ag_np.square(x_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a gradient function via `autograd.grad`\n",
    "\n",
    "Again, we can now create a gradient function by calling autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_D = np.zeros(4)\n",
    "print(x_D)\n",
    "print(f(x_D))\n",
    "print(g(x_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_D = np.asarray([1., 2., 3., 4., 5., 6.])\n",
    "print(x_D)\n",
    "print(f(x_D))\n",
    "print(g(x_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Check: Does the dimensionality of the above gradients make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Yourself: What do you predict the answer will be? What happened instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_D = np.asarray([11, -13])\n",
    "print(x_D)\n",
    "print(f(x_D))\n",
    "print(g(x_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Using autograd gradients within gradient descent to solve multivariate optimization problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function: basic gradient descent\n",
    "\n",
    "Here's a very simple function that will perform many gradient descent steps to optimize a given function.\n",
    "\n",
    "Make sure you understand its basic properties (the gradient descent algorithm is one of the prereqs of this course).\n",
    "\n",
    "Quick refresher: <https://en.wikipedia.org/wiki/Gradient_descent#Description>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_many_iters_of_gradient_descent(f, g, init_x_D=None, n_iters=100, step_size=0.001):\n",
    "\n",
    "    # Copy the initial parameter vector\n",
    "    x_D = copy.deepcopy(init_x_D)\n",
    "\n",
    "    # Create data structs to track the per-iteration history of different quantities\n",
    "    history = dict(\n",
    "        iter=[],\n",
    "        f=[],\n",
    "        x_D=[],\n",
    "        g_D=[])\n",
    "\n",
    "    # Perform many iterations of gradient descent updates\n",
    "    for iter_id in range(n_iters):\n",
    "        if iter_id > 0:\n",
    "            # Gradient descent update\n",
    "            x_D = x_D - step_size * g(x_D)\n",
    "\n",
    "        # After each update, store some info about the current state\n",
    "        history['iter'].append(iter_id)\n",
    "        history['f'].append(f(x_D))\n",
    "        history['x_D'].append(x_D)\n",
    "        history['g_D'].append(g(x_D))\n",
    "    return x_D, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example 3a: Minimize f(x) = sum(square(x))\n",
    "\n",
    "It's easy to figure out that the vector with smallest L2 norm (smallest sum of squares) is the all-zero vector.\n",
    "\n",
    "Here's a quick example of showing that using gradient functions provided by autograd can help us solve the optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_x  \\sum_{d=1}^D x_d^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x_D):\n",
    "    return ag_np.sum(ag_np.square(x_D))\n",
    "\n",
    "g = autograd.grad(f)\n",
    "\n",
    "# Initialize at x_D = [-3, 4, -5, 6]\n",
    "init_x_D = np.asarray([-3.0, 4.0, -5.0, 6.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x_D, history = run_many_iters_of_gradient_descent(f, g, init_x_D, n_iters=1000, step_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots of how x parameter values evolve over iterations, and function values evolve over iterations\n",
    "# Expected result: f goes to zero. all x values goto zero.\n",
    "\n",
    "fig_h, subplot_grid = plt.subplots(\n",
    "    nrows=1, ncols=2, sharex=True, sharey=False, figsize=(15,3), squeeze=False)\n",
    "subplot_grid[0,0].plot(history['iter'], history['x_D'])\n",
    "subplot_grid[0,0].set_xlabel('iters')\n",
    "subplot_grid[0,0].set_ylabel('x_d')\n",
    "\n",
    "subplot_grid[0,1].plot(history['iter'], history['f'])\n",
    "subplot_grid[0,1].set_xlabel('iters')\n",
    "subplot_grid[0,1].set_ylabel('f(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it Example 3b: Minimize the 'trid' function\n",
    "\n",
    "Given a 2-dimensional vector $x = [x_1, x_2]$, the trid function is:\n",
    "\n",
    "$$\n",
    "f(x) = (x_1-1)^2 + (x_2-1)^2 - x_1 x_2\n",
    "$$\n",
    "\n",
    "Background and Picture: <https://www.sfu.ca/~ssurjano/trid.html>\n",
    "\n",
    "Can you use autograd + gradient descent to find the optimal value $x^*$ that minimizes $f(x)$?\n",
    "\n",
    "You can initialize your gradient descent at [+1.0, -1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x_D):\n",
    "    return 0.0 # TODO\n",
    "\n",
    "g = f # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO call run_many_iters_of_gradient_descent() with appropriate args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRID example\n",
    "# Make plots of how x parameter values evolve over iterations, and function values evolve over iterations\n",
    "# Expected result: ????\n",
    "\n",
    "fig_h, subplot_grid = plt.subplots(\n",
    "    nrows=1, ncols=2, sharex=True, sharey=False, figsize=(15,3), squeeze=False)\n",
    "subplot_grid[0,0].plot(history['iter'], history['x_D'])\n",
    "subplot_grid[0,0].set_xlabel('iters')\n",
    "subplot_grid[0,0].set_ylabel('x_d')\n",
    "\n",
    "subplot_grid[0,1].plot(history['iter'], history['f'])\n",
    "subplot_grid[0,1].set_xlabel('iters')\n",
    "subplot_grid[0,1].set_ylabel('f(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Solving linear regression with gradient descent + autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Data for linear regression task\n",
    "\n",
    "We'll generate $N$ data examples (x_i, y_i)$ consisting of D-dimensional 'input' vectors $x_i$ and scalar outputs $y_i$.\n",
    "\n",
    "We'll use an idealized linear regression model.\n",
    "\n",
    "Each example has D=2 dimensions for x.\n",
    "\n",
    "The first dimension is weighted by +1.337\n",
    "The second dimension is weighted by -1.337\n",
    "\n",
    "The true \"bias\" will be 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300\n",
    "D = 2\n",
    "sigma = 0.1\n",
    "\n",
    "true_w_D = np.asarray([1.337, -1.337])\n",
    "true_bias = 10.0\n",
    "\n",
    "train_prng = np.random.RandomState(0)\n",
    "\n",
    "## Input data: uniform between (-5, 5)\n",
    "x_ND = train_prng.uniform(low=-5, high=5, size=(N,D))\n",
    "\n",
    "## True response: output of our \"true\" linear model\n",
    "f_N = np.dot(x_ND, true_w_D) + true_bias\n",
    "\n",
    "## Add small Gaussian noise\n",
    "noise_N = sigma * train_prng.randn(N)\n",
    "y_N = f_N + noise_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Data Visualization: Pairplots for all possible (x, y) scalar combinations\n",
    "\n",
    "We can\n",
    "* x1 vs y plot: slope is around +1 (should be +1.337)\n",
    "* x2 vs y plot: slope is around -1 (should be -1.337)\n",
    "* all y plots: average value (bias) is around +10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(\n",
    "    data=pd.DataFrame(np.hstack([x_ND, y_N[:,np.newaxis]]), columns=['x1', 'x2', 'y']));\n",
    "\n",
    "def hide_current_axis(*args, **kwds):\n",
    "    plt.gca().set_visible(False)\n",
    "g.map_upper(hide_current_axis);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try It Example 4a: Solve using a weights-only representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For now, even though the \"true bias\" is large, we'll consider only the weight coefficients (and assume the bias is zero)\n",
    "\n",
    "Consider the multivariate linear regression likelihood:\n",
    "\n",
    "\\begin{align}\n",
    "y_n &\\sim \\mathcal{N}(w^T x_n, \\sigma^2), \\forall n \\in 1, 2, \\ldots N\n",
    "\\end{align}\n",
    "where we assume $\\sigma = 0.1$.\n",
    "\n",
    "One non-Bayesian way to train weights would be to just compute the maximum likelihood solution:\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{w \\in \\mathbb{R}^2}  - \\log p(y | w, x)\n",
    "\\end{align}\n",
    "\n",
    "(Remember, maximizing likelihood  is the same as minimizing negative likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the optimization problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimization problem as an AUTOGRAD-able function wrt the weights w_D\n",
    "def calc_neg_likelihood_linreg(w_D):\n",
    "    return 0.5 / ag_np.square(sigma) * ag_np.sum(ag_np.square(ag_np.dot(x_ND, w_D) - y_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the function at an easy initial point\n",
    "init_w_D = np.zeros(2)\n",
    "calc_neg_likelihood_linreg(init_w_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the gradient at that easy point \n",
    "calc_grad_wrt_w = autograd.grad(calc_neg_likelihood_linreg)\n",
    "calc_grad_wrt_w(init_w_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the gradient's magnitude is very large, use very small step size\n",
    "opt_w_D, history = run_many_iters_of_gradient_descent(\n",
    "    calc_neg_likelihood_linreg, autograd.grad(calc_neg_likelihood_linreg), init_w_D,\n",
    "    n_iters=300, step_size=0.000001,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinReg worked example\n",
    "# Make plots of how w_D parameter values evolve over iterations, and function values evolve over iterations\n",
    "# Expected result: x\n",
    "\n",
    "fig_h, subplot_grid = plt.subplots(\n",
    "    nrows=1, ncols=2, sharex=True, sharey=False, figsize=(15,3), squeeze=False)\n",
    "subplot_grid[0,0].plot(history['iter'], [w_D[0] for w_D in history['x_D']], label='w_1')\n",
    "subplot_grid[0,0].plot(history['iter'], [w_D[1] for w_D in history['x_D']], label='w_2')\n",
    "\n",
    "subplot_grid[0,0].set_xlabel('iters')\n",
    "subplot_grid[0,0].set_ylabel('w_d')\n",
    "subplot_grid[0,0].legend()\n",
    "subplot_grid[0,0].set_ylim([-3, 3])\n",
    "\n",
    "subplot_grid[0,1].plot(history['iter'], history['f'])\n",
    "subplot_grid[0,1].set_xlabel('iters')\n",
    "subplot_grid[0,1].set_ylabel('-1 * log p(y | w, x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it Example 4b: Solve the linear regression problem using a weights-and-bias representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example only uses weights on the dimensions of $x_i$, and thus can only learn linear models that pass through the origin.\n",
    "\n",
    "Can you instead optimize a model that **also** includes a **bias** term $b>0$?\n",
    "\n",
    "\\begin{align}\n",
    "y_i &\\sim \\mathcal{N}(w^T x_i + b, \\sigma^2), \\forall i \\in 1, 2, \\ldots N\n",
    "\\end{align}\n",
    "where we assume $\\sigma = 0.1$.\n",
    "\n",
    "One non-Bayesian way to train weights would be to just compute the maximum likelihood solution:\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{w,b}  - \\log p(y | w, b, x)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way to do this is to imagine that each observation vector $x_i$ is expanded into a $\\tilde{x}_i$ that contains a column of all ones.  Then, we can write the corresponding expanded weights as $\\tilde{w} = [w_1 w_2 b]$.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{\\tilde{w} \\in \\mathbb{R}^3}  - \\log p(y | \\tilde{w},\\tilde{x})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, each expanded xtilde vector has size E = D+1 = 3\n",
    "\n",
    "xtilde_NE = np.hstack([x_ND, np.ones((N,1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define f to minimize that takes a COMBINED weights-and-bias vector wtilde_E of size E=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradient of f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO run gradient descent and plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 setup: Autograd for functions of data structures of arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful Fact: autograd can take derivatives with respect to DATA STRUCTURES of parameters\n",
    "\n",
    "This can help us when it is natural to define models in terms of several parts (e.g. NN layers).\n",
    "\n",
    "We don't need to turn our many model parameters into one giant weights-and-biases vector. We can express our thoughts more naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1: gradient of a LIST of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w_list_of_arr):\n",
    "    return ag_np.sum(ag_np.square(w_list_of_arr[0])) + ag_np.sum(ag_np.square(w_list_of_arr[1]))\n",
    "\n",
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list_of_arr = [np.zeros(3), np.arange(5, dtype=np.float64)]\n",
    "\n",
    "print(\"Type of the gradient is: \")\n",
    "print(type(g(w_list_of_arr)))\n",
    "\n",
    "print(\"Result of the gradient is: \")\n",
    "g(w_list_of_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2: gradient of DICT of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(dict_of_arr):\n",
    "    return ag_np.sum(ag_np.square(dict_of_arr['weights'])) + ag_np.sum(ag_np.square(dict_of_arr['bias']))\n",
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_arr = dict(weights=np.arange(5, dtype=np.float64), bias=4.2)\n",
    "\n",
    "print(\"Type of the gradient is: \")\n",
    "print(type(g(dict_of_arr)))\n",
    "\n",
    "print(\"Result of the gradient is: \")\n",
    "g(dict_of_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Neural Networks and Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use a convenient data structure for NN model parameters\n",
    "\n",
    "Use a list of dicts of arrays.\n",
    "\n",
    "Each entry in the list is a dict that represents the parameters of one \"layer\".\n",
    "\n",
    "Each layer-specific dict has two named attributes: a vector of weights 'w' and a vector of biases 'b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's a function to create NN params as a 'list-of-dicts' that match a provided set of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn_params_as_list_of_dicts(\n",
    "        n_hiddens_per_layer_list=[5],\n",
    "        n_dims_input=1,\n",
    "        n_dims_output=1,\n",
    "        weight_fill_func=np.zeros,\n",
    "        bias_fill_func=np.zeros):\n",
    "    nn_param_list = []\n",
    "    n_hiddens_per_layer_list = [n_dims_input] + n_hiddens_per_layer_list + [n_dims_output]\n",
    "\n",
    "    # Given full network size list is [a, b, c, d, e]\n",
    "    # For loop should loop over (a,b) , (b,c) , (c,d) , (d,e)\n",
    "    for n_in, n_out in zip(n_hiddens_per_layer_list[:-1], n_hiddens_per_layer_list[1:]):\n",
    "        nn_param_list.append(\n",
    "            dict(\n",
    "                w=weight_fill_func((n_in, n_out)),\n",
    "                b=bias_fill_func((n_out,)),\n",
    "            ))\n",
    "    return nn_param_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's a function to pretty-print any given set of NN parameters to stdout, so we can inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_nn_param_list(nn_param_list_of_dict):\n",
    "    \"\"\" Create pretty display of the parameters at each layer\n",
    "    \"\"\"\n",
    "    for ll, layer_dict in enumerate(nn_param_list_of_dict):\n",
    "        print(\"Layer %d\" % ll)\n",
    "        print(\"  w | size %9s | %s\" % (layer_dict['w'].shape, layer_dict['w'].flatten()))\n",
    "        print(\"  b | size %9s | %s\" % (layer_dict['b'].shape, layer_dict['b'].flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: NN with 0 hidden layers (equivalent to linear regression)\n",
    "\n",
    "For univariate regression: 1D -> 1D\n",
    "\n",
    "Will fill all parameters with zeros by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = make_nn_params_as_list_of_dicts(n_hiddens_per_layer_list=[], n_dims_input=1, n_dims_output=1)\n",
    "pretty_print_nn_param_list(nn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: NN with 0 hidden layers (equivalent to linear regression)\n",
    "\n",
    "For multivariate regression when |x_i| = 2: 2D -> 1D\n",
    "\n",
    "Will fill all parameters with zeros by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = make_nn_params_as_list_of_dicts(n_hiddens_per_layer_list=[], n_dims_input=2, n_dims_output=1)\n",
    "pretty_print_nn_param_list(nn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: NN with 1 hidden layer of 3 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = make_nn_params_as_list_of_dicts(n_hiddens_per_layer_list=[3], n_dims_input=2, n_dims_output=1)\n",
    "pretty_print_nn_param_list(nn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: NN with 1 hidden layer of 3 hidden units\n",
    "\n",
    "Use 'ones' as the fill function for weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = make_nn_params_as_list_of_dicts(\n",
    "    n_hiddens_per_layer_list=[3], n_dims_input=2, n_dims_output=1,\n",
    "    weight_fill_func=np.ones)\n",
    "pretty_print_nn_param_list(nn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: NN with 1 hidden layer of 3 hidden units\n",
    "\n",
    "Use random draws from standard normal as the fill function for weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = make_nn_params_as_list_of_dicts(\n",
    "    n_hiddens_per_layer_list=[3], n_dims_input=2, n_dims_output=1,\n",
    "    weight_fill_func=lambda size_tuple: np.random.randn(*size_tuple))\n",
    "pretty_print_nn_param_list(nn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: NN with 7 hidden layers of diff sizes\n",
    "\n",
    "Just shows how generic this framework is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = make_nn_params_as_list_of_dicts(\n",
    "    n_hiddens_per_layer_list=[3, 4, 5, 6, 5, 4, 3], n_dims_input=2, n_dims_output=1)\n",
    "pretty_print_nn_param_list(nn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Function that performs **prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y_given_x_with_NN(x=None, nn_param_list=None, activation_func=ag_np.tanh):\n",
    "    \"\"\" Predict y value given x value via feed-forward neural net\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    x : array_like, n_examples x n_input_dims\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : array_like, n_examples\n",
    "    \"\"\"\n",
    "    for layer_id, layer_dict in enumerate(nn_param_list):\n",
    "        if layer_id == 0:\n",
    "            if x.ndim > 1:\n",
    "                in_arr = x\n",
    "            else:\n",
    "                if x.size == nn_param_list[0]['w'].shape[0]:\n",
    "                    in_arr = x[ag_np.newaxis,:]\n",
    "                else:\n",
    "                    in_arr = x[:,ag_np.newaxis]                    \n",
    "        else:\n",
    "            in_arr = activation_func(out_arr)\n",
    "        out_arr = ag_np.dot(in_arr, layer_dict['w']) + layer_dict['b']\n",
    "    return ag_np.squeeze(out_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Make predictions with 0-layer NN whose parameters  are filled with the 'true' params for our toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_nn_params = make_nn_params_as_list_of_dicts(n_hiddens_per_layer_list=[], n_dims_input=2, n_dims_output=1)\n",
    "true_nn_params[0]['w'][:] = true_w_D[:,np.newaxis]\n",
    "true_nn_params[0]['b'][:] = true_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_N = predict_y_given_x_with_NN(x_ND, true_nn_params)\n",
    "assert yhat_N.size == N\n",
    "\n",
    "plt.plot(yhat_N, y_N, 'k.')\n",
    "plt.xlabel('true y')\n",
    "plt.ylabel('predicted y|x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Make predictions with 0-layer NN whose parameters  are filled with all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_nn_params = make_nn_params_as_list_of_dicts(n_hiddens_per_layer_list=[], n_dims_input=2, n_dims_output=1)\n",
    "yhat_N = predict_y_given_x_with_NN(x_ND, zero_nn_params)\n",
    "assert yhat_N.size == N\n",
    "\n",
    "plt.plot(yhat_N, y_N, 'k.')\n",
    "plt.xlabel('true y')\n",
    "plt.ylabel('predicted y|x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Gradient descent implementation that can use list-of-dict parameters (not just arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_many_iters_of_gradient_descent_with_list_of_dict(\n",
    "        f, g, init_x_list_of_dict=None,\n",
    "        n_iters=100, step_size=0.001,\n",
    "        n_steps_between_print=50):\n",
    "\n",
    "    # Copy the initial parameter vector\n",
    "    x_list_of_dict = copy.deepcopy(init_x_list_of_dict)\n",
    "\n",
    "    # Create data structs to track the per-iteration history of different quantities\n",
    "    history = dict(\n",
    "        iter=[],\n",
    "        f=[],\n",
    "        x=[],\n",
    "        g=[])\n",
    "    start_time = time.time()\n",
    "    for iter_id in range(n_iters):\n",
    "        if iter_id > 0:\n",
    "            # Gradient is a list of layer-specific dicts\n",
    "            grad_list_of_dict = g(x_list_of_dict)\n",
    "            for layer_id, x_layer_dict in enumerate(x_list_of_dict):\n",
    "                for key in x_layer_dict.keys():\n",
    "                    x_layer_dict[key] = x_layer_dict[key] - step_size * grad_list_of_dict[layer_id][key]\n",
    "                    \n",
    "        fval = f(x_list_of_dict)\n",
    "        history['iter'].append(iter_id)\n",
    "        history['f'].append(fval)\n",
    "        history['x'].append(copy.deepcopy(x_list_of_dict))\n",
    "        history['g'].append(g(x_list_of_dict))\n",
    "\n",
    "        if iter_id < 3 or (iter_id+1) % n_steps_between_print == 0:\n",
    "            print(\"completed iter %5d/%d after %7.1f sec | loss %.6e\" % (\n",
    "                iter_id+1, n_iters, time.time()-start_time, fval))\n",
    "    return x_list_of_dict, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worked Exercise 5a: Train 0-layer NN via gradient descent on LINEAR toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_regression_loss_function(nn_params):\n",
    "    yhat_N = predict_y_given_x_with_NN(x_ND, nn_params)\n",
    "    return 0.5 / ag_np.square(sigma) * ag_np.sum(ag_np.square(y_N - yhat_N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5a(i): Try first by starting the optimization at the IDEAL answer \n",
    "\n",
    "Why? Good debugging strategy! Make sure your optimization and loss function are set up correctly. If all goes well, if we start at an intended optimal point, we shouldn't go very far (just perhaps micro optimizing for small sample effects.... the true bias is +10 but with our 100 samples maybe the best to do is +9.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromtrue_opt_nn_params, fromtrue_history = run_many_iters_of_gradient_descent_with_list_of_dict(\n",
    "    nn_regression_loss_function,\n",
    "    autograd.grad(nn_regression_loss_function),\n",
    "    true_nn_params,\n",
    "    n_iters=100,\n",
    "    step_size=0.000001,\n",
    "    n_steps_between_print=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty print the learned weights and bias... do they match the true ones used to generate the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_nn_param_list(fromtrue_opt_nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fromtrue_history['iter'], fromtrue_history['f'], 'k.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5a(ii): Try  by starting the optimization at ALL ZEROS (so it needs to go somewhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromzero_opt_nn_params, fromzero_history = run_many_iters_of_gradient_descent_with_list_of_dict(\n",
    "    nn_regression_loss_function,\n",
    "    autograd.grad(nn_regression_loss_function),\n",
    "    zero_nn_params,\n",
    "    n_iters=100,\n",
    "    step_size=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_nn_param_list(fromzero_opt_nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fromzero_history['iter'], fromzero_history['f'], 'k.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5b: Create more complex non-linear toy dataset\n",
    "\n",
    "True method *regression from QUADRATIC features*:\n",
    "\n",
    "$$\n",
    "y \\sim \\text{Normal}( w_1 x_1 + w_2 x_2 + w_3 x_1^2 + w_4 x_2^2 + b, \\sigma^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300\n",
    "D = 2\n",
    "sigma = 0.1\n",
    "\n",
    "wsq_D = np.asarray([-2.0, 2.0])\n",
    "w_D = np.asarray([4.2, -4.2])\n",
    "\n",
    "train_prng = np.random.RandomState(0)\n",
    "x_ND = train_prng.uniform(low=-5, high=5, size=(N,D))\n",
    "y_N = (\n",
    "    np.dot(np.square(x_ND), wsq_D)\n",
    "    + np.dot(x_ND, w_D)\n",
    "    + sigma * train_prng.randn(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    data=pd.DataFrame(np.hstack([x_ND, y_N[:,np.newaxis]]), columns=['x1', 'x2', 'y']));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_toy_nn_regression_loss_function(nn_params):\n",
    "    yhat_N = predict_y_given_x_with_NN(x_ND, nn_params)\n",
    "    return 0.5 / ag_np.square(sigma) * ag_np.sum(ag_np.square(y_N - yhat_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 1-layer, 10 hidden unit network with small random noise on weights\n",
    "\n",
    "H10_init_nn_params = make_nn_params_as_list_of_dicts(\n",
    "    n_hiddens_per_layer_list=[10], n_dims_input=2, n_dims_output=1,\n",
    "    weight_fill_func=lambda sz_tuple: 0.1 * np.random.randn(*sz_tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H10_opt_nn_params, H10_history = run_many_iters_of_gradient_descent_with_list_of_dict(\n",
    "    nonlinear_toy_nn_regression_loss_function,\n",
    "    autograd.grad(nonlinear_toy_nn_regression_loss_function),\n",
    "    H10_init_nn_params,\n",
    "    n_iters=300,\n",
    "    step_size=0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot objective function vs iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H10_history['iter'], H10_history['f'], 'k.-')\n",
    "plt.title('10 hidden units');\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('loss (negative log proba. of y)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot predicted y vs. true y for each example as a scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_N = predict_y_given_x_with_NN(x_ND, H10_opt_nn_params)\n",
    "\n",
    "plt.plot(yhat_N, y_N, 'k.');\n",
    "plt.xlabel('predicted y|x');\n",
    "plt.ylabel('true y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, subplot_grid = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=False, figsize=(10,3), squeeze=False)\n",
    "subplot_grid[0,0].plot(x_ND[:,0], y_N, 'k.');\n",
    "subplot_grid[0,0].plot(x_ND[:,0], yhat_N, 'b.')\n",
    "subplot_grid[0,0].set_xlabel('x_0');\n",
    "\n",
    "subplot_grid[0,1].plot(x_ND[:,1], y_N, 'k.');\n",
    "subplot_grid[0,1].plot(x_ND[:,1], yhat_N, 'b.')\n",
    "subplot_grid[0,1].set_xlabel('x_1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More units! Try 1 layer with H=30 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 1-layer, 30 hidden unit network with small random noise on weights\n",
    "H30_init_nn_params = make_nn_params_as_list_of_dicts(\n",
    "    n_hiddens_per_layer_list=[30], n_dims_input=2, n_dims_output=1,\n",
    "    weight_fill_func=lambda sz_tuple: 0.1 * np.random.randn(*sz_tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H30_opt_nn_params, H30_history = run_many_iters_of_gradient_descent_with_list_of_dict(\n",
    "    nonlinear_toy_nn_regression_loss_function,\n",
    "    autograd.grad(nonlinear_toy_nn_regression_loss_function),\n",
    "    H30_init_nn_params,\n",
    "    n_iters=300,\n",
    "    step_size=0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot objective function vs iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H30_history['iter'], H30_history['f'], 'k.-');\n",
    "plt.title('30 hidden units');\n",
    "\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('loss (negative log proba. of y)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot predicted y value vs true y value for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_N = predict_y_given_x_with_NN(x_ND, H30_opt_nn_params)\n",
    "\n",
    "plt.plot(yhat_N, y_N, 'k.');\n",
    "plt.xlabel('predicted y|x');\n",
    "plt.ylabel('true y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, subplot_grid = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=False, figsize=(10,3), squeeze=False)\n",
    "subplot_grid[0,0].plot(x_ND[:,0], y_N, 'k.');\n",
    "subplot_grid[0,0].plot(x_ND[:,0], yhat_N, 'b.')\n",
    "subplot_grid[0,0].set_xlabel('x_0');\n",
    "\n",
    "subplot_grid[0,1].plot(x_ND[:,1], y_N, 'k.');\n",
    "subplot_grid[0,1].plot(x_ND[:,1], yhat_N, 'b.')\n",
    "subplot_grid[0,1].set_xlabel('x_1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even more units! Try 1 layer with H=100 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 1-layer, 100 hidden unit network with small random noise on weights\n",
    "H100_init_nn_params = make_nn_params_as_list_of_dicts(\n",
    "    n_hiddens_per_layer_list=[100], n_dims_input=2, n_dims_output=1,\n",
    "    weight_fill_func=lambda sz_tuple: 0.05 * np.random.randn(*sz_tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H100_opt_nn_params, H100_history = run_many_iters_of_gradient_descent_with_list_of_dict(\n",
    "    nonlinear_toy_nn_regression_loss_function,\n",
    "    autograd.grad(nonlinear_toy_nn_regression_loss_function),\n",
    "    H100_init_nn_params,\n",
    "    n_iters=600,\n",
    "    step_size=0.0000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_N = predict_y_given_x_with_NN(x_ND, H100_opt_nn_params)\n",
    "\n",
    "plt.plot(yhat_N, y_N, 'k.');\n",
    "plt.xlabel('predicted y|x');\n",
    "plt.ylabel('true y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, subplot_grid = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=False, figsize=(10,3), squeeze=False)\n",
    "subplot_grid[0,0].plot(x_ND[:,0], y_N, 'k.');\n",
    "subplot_grid[0,0].plot(x_ND[:,0], yhat_N, 'b.')\n",
    "subplot_grid[0,0].set_xlabel('x_0');\n",
    "\n",
    "subplot_grid[0,1].plot(x_ND[:,1], y_N, 'k.');\n",
    "subplot_grid[0,1].plot(x_ND[:,1], yhat_N, 'b.')\n",
    "subplot_grid[0,1].set_xlabel('x_1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it yourself!\n",
    "\n",
    "* Can you train a prediction network on the non-linear toy data so it has ZERO training error? Is this even possible?\n",
    "\n",
    "* Can you make the network train faster? What happens if you play with the step_size?\n",
    "\n",
    "* What if you made the network **deeper** (more layers)?\n",
    "\n",
    "* What other dataset would you want to try out this regression on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
